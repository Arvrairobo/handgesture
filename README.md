[Home](/README.md) | [Reference](/reference.md) | [Edit Guide](/editguide.md) | <button class="nav" ><a href="https://github.com/whatifif/handgesture/">View on Github</a></button>  |  <button class="nav" ><a href="https://whatifif.github.io/handgesture/">View on Web</a></button>


## TODOs

### get more data of hand gesture.

### connecting a trained model to recognise a gesture in main program ( streamlined at real time)

### make a demo program
   1. caculation program
   (2. running dinosaur )
  
### prepare a presentation
   1. introduction
   2. creating data sets
   3. training and testing
   4  demo part
   5. summary and future work

### tracking a right hand for mouse



## Controlling a Computer by Hand Gesture

It would be nice to controll a computer by hand gesture and/or voice remotely without using a keyboard and a mouse. 

This can make people freed from the keyboard and mouse, solving the health problem caused by sitting steady for a long time.

Additionally this makes game-players feel more immersive in a game by using their body for interaction.

Currently VR (virtual reality) and AR (augmented reality) use a special controller to replace the keyboard and the mouse for performance reason.

Ideally these controller will be replaced with hand gestures and voice commands.

There have been many researches about this hand gesture recognition. What we are going to achieve is to cover all the keys on a keyboard with as many hand gestures as possible. 

## Recent works in this field

1. [Carnegie Mellon University OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose): webcam

2. [Microsoft hololens](https://www.microsoft.com/en-au/hololens): 3D sensor

3. [Microsoft hand tracking](https://www.microsoft.com/en-us/research/project/fully-articulated-hand-tracking/): 3D sensor

4. [Leap Motion](https://www.leapmotion.com): 3D sensor

5. [Mano Motion](https://www.manomotion.com/): smartphone camera

6. [PilotBit Mobile Hand Tracking](http://www.pilotbit.com/): 3D sensor


## Chalenging points

1. using a webcam not using 3D sensor (depth camera)

2. detecting the subtle gestures at real time

3. deep learning running at smartphone


## Work flow

1. defining as many hand gestures as possible to cover all the keys on a keyboard.

2. creating many data sets accordingly.

3. searching for a deep learning model which can track and recognise these hand gestures at real time.

4. coding for hand gesture recognition

5. coding for hand tracking

6. creating some demos such as basic calculation or a game.

## Why MxNet?


## How can the trained model be transferred to work at smartphone?


## Is the detection area ( Region of Interest ) for hand needed?




### Markdown

Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for

```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).

### Jekyll Themes

Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/whatifif/handgesture/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

### Support or Contact

Having trouble with Pages? Check out our [documentation](https://help.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and weâ€™ll help you sort it out.

### [LICENSE](/LICENSE)
GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007

Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
 
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.
