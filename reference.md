[Home](/README.md) | [Reference](/reference.md) | [Edit Guide](/editguide.md) | <button class="nav" ><a href="https://github.com/whatifif/handgesture/">View on Github</a></button>  |  <button class="nav" ><a href="https://whatifif.github.io/handgesture/">View on Web</a></button>

### Reference

[manomotion](https://www.manomotion.com/get-started/) : real-time 3D gestural analysis. 
ManoMotion provides a framework for real- time 3D gestural analysis. Minimal hardware, minimal computing power. All thatâ€™s required is a simple RGB camera found in everyday smartphone.


[yolo](https://pjreddie.com/darknet/yolo/) : real-time object detection


[universe](https://blog.openai.com/universe/) : Universe allows an AI agent to use a computer like a human does: by looking at screen pixels and operating a virtual keyboard and mouse. 

[https://github.com/opencv/opencv](https://github.com/opencv/opencv)

[http://opencv.org/](http://opencv.org/)

[https://pjreddie.com/darknet/](https://pjreddie.com/darknet/)

[Opencv python hand gesture recognition](Opencv python hand gesture recognition): opencv with python
